{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you need to tackle a complex problem, such as detecting hundreds of types of objects in high-resolution images? You may need to train a much deeper DNN\n",
    "\n",
    "Training a deep DNN isn’t a walk in the park. Here are\n",
    "some of the problems you could run into :\n",
    "\n",
    "- You may be faced with the tricky *vanishing gradients* (gradient decreasing close to 0 can't update Weight) problem or the related *exploding gradients* (gradient increasing to infiny or NAN) problem.\n",
    "- Training may be extremely slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter we will go through each of these problems and present\n",
    "techniques to solve them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Vanishing/Exploding Gradients Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the combination of the popular logistic sigmoid activation function and\n",
    "the weight initialization technique that was most popular at the time (i.e.,\n",
    "a normal distribution with a mean of 0 and a standard deviation of 1). the fact\n",
    "that the logistic function has a mean of 0.5, not 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z) :\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='satur.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> the function saturates at 0 or 1 \n",
    "\n",
    "> Thus, when backpropagation kicks in it has virtually no gradient to propagate backthrough the network; and what little gradient exists keeps getting **diluted as backpropagation** progresses down through the top layers, so there is **really nothing left for the lower layers**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Glorot(Xavier) and He Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a way to significantly alleviate the unstable gradients problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"init.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> By default, Keras uses Glorot initialization with a uniform distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Constant',\n",
       " 'GlorotNormal',\n",
       " 'GlorotUniform',\n",
       " 'HeNormal',\n",
       " 'HeUniform',\n",
       " 'Identity',\n",
       " 'Initializer',\n",
       " 'LecunNormal',\n",
       " 'LecunUniform',\n",
       " 'Ones',\n",
       " 'Orthogonal',\n",
       " 'RandomNormal',\n",
       " 'RandomUniform',\n",
       " 'TruncatedNormal',\n",
       " 'VarianceScaling',\n",
       " 'Zeros',\n",
       " 'constant',\n",
       " 'deserialize',\n",
       " 'get',\n",
       " 'glorot_normal',\n",
       " 'glorot_uniform',\n",
       " 'he_normal',\n",
       " 'he_uniform',\n",
       " 'identity',\n",
       " 'lecun_normal',\n",
       " 'lecun_uniform',\n",
       " 'ones',\n",
       " 'orthogonal',\n",
       " 'random_normal',\n",
       " 'random_uniform',\n",
       " 'serialize',\n",
       " 'truncated_normal',\n",
       " 'variance_scaling',\n",
       " 'zeros']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[name for name in dir(keras.initializers) if not name.startswith(\"_\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Glorot and Bengio proposed a good compromise that has\n",
    "proven to work very well in practice: the connection weights of each\n",
    "layer must be initialized randomly as described in Equation 11-1, where\n",
    "fan-avg = (fan-in + fan-out )/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Fan-in: is a term that defines the maximum number of inputs that a system can accept. \n",
    "> - Fan-out: is a term that defines the maximum number of inputs that the output of a system can feed to other systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Glorot initialization can speed up training considerably, and it is\n",
    "one of the tricks that led to the success of Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.dense.Dense at 0x1f278b5d3c0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(10, activation='relu', kernel_initializer='he_normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.dense.Dense at 0x1f278c348b0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init = keras.initializers.VarianceScaling(scale=.2, mode='fan_avg', distribution='uniform') # set custom initialze random weight\n",
    "keras.layers.Dense(10, activation='relu', kernel_initializer=init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonsaturating Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Glorot and Bengio was that the\n",
    "problems with unstable gradients were in part due to a poor choice of activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU activation function, mostly because it does not saturate for positive values (and because it is fast to compute)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ReLU activation function is not perfect. It suffers from\n",
    "a problem known as the dying ReLUs: during training, some neurons effectively “die,” meaning they stop outputting anything other than 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve this problem, you may want to use a variant of the ReLU function, such as the leaky ReLU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(z, alpha=0.01) :\n",
    "    return np.maximum(alpha*z, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The hyperparameter α defines how much the function 'leaks': ensures that leaky ReLUs never die;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "paper also evaluated the randomized leaky ReLU (RReLU), where α is picked randomly, seemed to act as a regularizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the paper evaluated the parametric leaky ReLU (PReLU), where α is authorized to be learned during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> PReLU was reported to strongly outperform ReLU on large image datasets, but on smaller datasets it runs the risk of overfitting the training set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "activation function called the exponential linear unit (ELU) that outperformed all the ReLU variants (slower to\n",
    "compute than the ReLU function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Günter Klambauer et al. introduced the Scaled\n",
    "ELU (SELU) activation function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> SELU activation\n",
    "function often significantly outperforms other activation functions for\n",
    "such neural nets (will selfnormalize: the output of each layer will tend to preserve a mean of 0 and \n",
    "standard deviation of 1 during training, which solves the\n",
    "vanishing/exploding gradients problem) [The network’s architecture must be sequential. Unfortunately, if you\n",
    "try to use SELU in nonsequential architectures, such as recurrent networks will not necessarily outperform other\n",
    "activation functions]\n",
    "\n",
    "> - The input features must be standardized (mean 0 and standard deviation 1).\n",
    "> - Every hidden layer’s weights must be initialized with LeCun normal\n",
    "initialization. In Keras, this means setting\n",
    "kernel_initializer=\"lecun_normal\" .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in general SELU > ELU > leaky\n",
    "ReLU (and its variants) > ReLU > tanh > logistic\n",
    "- If the network’s architecture prevents it from self-normalizing, then ELU\n",
    "-  care a lot about runtime latency, then you may\n",
    "prefer leaky ReLU. (0.3 for leaky ReLU)\n",
    "- If you have spare time and computing power, you can use cross-validation to evaluate other\n",
    "activation functions, such as RReLU if your network is overfitting or PReLU if you\n",
    "have a huge training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> if speed is your priority, ReLU might still be the best choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"leak.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['deserialize',\n",
       " 'elu',\n",
       " 'exponential',\n",
       " 'gelu',\n",
       " 'get',\n",
       " 'hard_sigmoid',\n",
       " 'linear',\n",
       " 'relu',\n",
       " 'selu',\n",
       " 'serialize',\n",
       " 'sigmoid',\n",
       " 'softmax',\n",
       " 'softplus',\n",
       " 'softsign',\n",
       " 'swish',\n",
       " 'tanh']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[activation for activation in dir(keras.activations) if not activation.startswith(\"_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LeakyReLU', 'PReLU', 'ReLU', 'ThresholdedReLU']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[layer for layer in dir(keras.layers) if \"relu\" in layer.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.dense.Dense at 0x1f278c340a0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for use SELU\n",
    "layer = keras.layers.Dense(10, activation=\"selu\", kernel_initializer=\"lecun_normal\")\n",
    "layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization\n",
    "scale it to 0 mean and 1 variance and  scale it add plus Bias term(called offset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although using He initialization along with ELU (or any variant of ReLU) can significantly reduce the danger of the vanishing/exploding gradients problems ( doesn’t guarantee that they\n",
    "won’t come back during training.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adding an operation in the model just before or after the activation function of each hidden layer\n",
    "\n",
    "<img src=\"bn.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source : https://www.youtube.com/watch?v=yXOMHOpbon8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the operation lets the model learn the optimal\n",
    "scale and mean of each of the layer’s inputs.\n",
    "\n",
    "if you add a\n",
    "BN layer as the very first layer of your neural network, you do not need to\n",
    "standardize your training set (The vanishing gradients problem was\n",
    "strongly reduced, to the point that they could use saturating activation\n",
    "functions such as the tanh and even the logistic activation function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, there is a runtime penalty: the neural network makes slower predictions due to the extra computations required at\n",
    "each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.13832426 0.38750803 0.09253335]\n",
      " [0.7931794  0.17145753 0.38980567]\n",
      " [0.57801473 0.13108122 0.6219201 ]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "image_tensor = tf.random.uniform(shape=(3,3), minval=0, maxval=1)\n",
    "print(image_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.13825515 0.38731444 0.09248712]\n",
      " [0.79278314 0.17137186 0.38961092]\n",
      " [0.57772595 0.13101573 0.6216094 ]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "bn_layers = keras.layers.BatchNormalization()(image_tensor)\n",
    "print(bn_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(), # do not to do manually normalize\n",
    "    keras.layers.Dense(300, activation=\"relu\" , use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, use_bias=False),\n",
    "    keras.layers.BatchNormalization(), # add BN before activation\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> the layer before a BatchNormalization layer does not need to have bias terms, since the BatchNormalization layer has some as well, it would be a waste of parameters, so you can set use_bias=False when creating those layers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The authors of the BN paper argued in favor of adding the BN layers before the activation functions, rather than after\n",
    "\n",
    "> you can experiment with this too to see which option works best on\n",
    "your dataset. To add the BN layers before the activation functions, you\n",
    "must remove the activation function from the hidden layers and add\n",
    "them as separate layers after the BN layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_26 (Flatten)        (None, 784)               0         \n",
      "                                                                 \n",
      " batch_normalization_35 (Bat  (None, 784)              3136      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 300)               235200    \n",
      "                                                                 \n",
      " batch_normalization_36 (Bat  (None, 300)              1200      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 100)               30000     \n",
      "                                                                 \n",
      " batch_normalization_37 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 270,946\n",
      "Trainable params: 268,578\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each BN layer adds four parameters per input: γ, β, μ, and\n",
    "σ (for example, the first BN layer adds 3,136 parameters, which is 4 × 784)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The last two parameters, μ and σ, are the moving averages; they are not\n",
    "affected by backpropagation, so Keras calls them “non-trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batch_normalization_35/gamma:0', True),\n",
       " ('batch_normalization_35/beta:0', True),\n",
       " ('batch_normalization_35/moving_mean:0', False),\n",
       " ('batch_normalization_35/moving_variance:0', False)]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn1 = model.layers[1]\n",
    "[(var.name, var.trainable) for var in bn1.variables]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another popular technique to mitigate the exploding gradients problem is\n",
    "to clip the gradients during backpropagation so that they never exceed\n",
    "some threshold\n",
    "\n",
    "This technique is most\n",
    "often used in recurrent neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.optimizer_v2.gradient_descent.SGD at 0x1f279cb1f60>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just a matter of setting the clipvalue or clipnorm argument when creating an optimizer\n",
    "keras.optimizers.SGD(clipvalue=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> clipvalue (float) is set, the gradient of each weight is clipped to be no higher than this value (maximum value set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reusing Pretrained Layers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63e79917a05e390872358bfb73c58bc903ada01d2d04077091749088207d82cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
