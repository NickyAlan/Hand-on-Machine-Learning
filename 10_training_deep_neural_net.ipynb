{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you need to tackle a complex problem, such as detecting hundreds of types of objects in high-resolution images? You may need to train a much deeper DNN\n",
    "\n",
    "Training a deep DNN isn’t a walk in the park. Here are\n",
    "some of the problems you could run into :\n",
    "\n",
    "- You may be faced with the tricky *vanishing gradients* (gradient decreasing close to 0 can't update Weight) problem or the related *exploding gradients* (gradient increasing to infiny or NAN) problem.\n",
    "- Training may be extremely slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter we will go through each of these problems and present\n",
    "techniques to solve them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Vanishing/Exploding Gradients Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the combination of the popular logistic sigmoid activation function and\n",
    "the weight initialization technique that was most popular at the time (i.e.,\n",
    "a normal distribution with a mean of 0 and a standard deviation of 1). the fact\n",
    "that the logistic function has a mean of 0.5, not 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z) :\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='satur.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> the function saturates at 0 or 1 \n",
    "\n",
    "> Thus, when backpropagation kicks in it has virtually no gradient to propagate backthrough the network; and what little gradient exists keeps getting **diluted as backpropagation** progresses down through the top layers, so there is **really nothing left for the lower layers**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Glorot(Xavier) and He Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a way to significantly alleviate the unstable gradients problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"init.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> By default, Keras uses Glorot initialization with a uniform distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Constant',\n",
       " 'GlorotNormal',\n",
       " 'GlorotUniform',\n",
       " 'HeNormal',\n",
       " 'HeUniform',\n",
       " 'Identity',\n",
       " 'Initializer',\n",
       " 'LecunNormal',\n",
       " 'LecunUniform',\n",
       " 'Ones',\n",
       " 'Orthogonal',\n",
       " 'RandomNormal',\n",
       " 'RandomUniform',\n",
       " 'TruncatedNormal',\n",
       " 'VarianceScaling',\n",
       " 'Zeros',\n",
       " 'constant',\n",
       " 'deserialize',\n",
       " 'get',\n",
       " 'glorot_normal',\n",
       " 'glorot_uniform',\n",
       " 'he_normal',\n",
       " 'he_uniform',\n",
       " 'identity',\n",
       " 'lecun_normal',\n",
       " 'lecun_uniform',\n",
       " 'ones',\n",
       " 'orthogonal',\n",
       " 'random_normal',\n",
       " 'random_uniform',\n",
       " 'serialize',\n",
       " 'truncated_normal',\n",
       " 'variance_scaling',\n",
       " 'zeros']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[name for name in dir(keras.initializers) if not name.startswith(\"_\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Glorot and Bengio proposed a good compromise that has\n",
    "proven to work very well in practice: the connection weights of each\n",
    "layer must be initialized randomly as described in Equation 11-1, where\n",
    "fan-avg = (fan-in + fan-out )/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Fan-in: is a term that defines the maximum number of inputs that a system can accept. \n",
    "> - Fan-out: is a term that defines the maximum number of inputs that the output of a system can feed to other systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Glorot initialization can speed up training considerably, and it is\n",
    "one of the tricks that led to the success of Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.dense.Dense at 0x273c0cdd360>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(10, activation='relu', kernel_initializer='he_normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.dense.Dense at 0x273c0cdfe20>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init = keras.initializers.VarianceScaling(scale=.2, mode='fan_avg', distribution='uniform') # set custom initialze random weight\n",
    "keras.layers.Dense(10, activation='relu', kernel_initializer=init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonsaturating Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Glorot and Bengio was that the\n",
    "problems with unstable gradients were in part due to a poor choice of activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU activation function, mostly because it does not saturate for positive values (and because it is fast to compute)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ReLU activation function is not perfect. It suffers from\n",
    "a problem known as the dying ReLUs: during training, some neurons effectively “die,” meaning they stop outputting anything other than 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve this problem, you may want to use a variant of the ReLU function, such as the leaky ReLU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(z, alpha=0.01) :\n",
    "    return np.maximum(alpha*z, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The hyperparameter α defines how much the function 'leaks': ensures that leaky ReLUs never die;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "paper also evaluated the randomized leaky ReLU (RReLU), where α is picked randomly, seemed to act as a regularizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the paper evaluated the parametric leaky ReLU (PReLU), where α is authorized to be learned during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> PReLU was reported to strongly outperform ReLU on large image datasets, but on smaller datasets it runs the risk of overfitting the training set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "activation function called the exponential linear unit (ELU) that outperformed all the ReLU variants (slower to\n",
    "compute than the ReLU function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Günter Klambauer et al. introduced the Scaled\n",
    "ELU (SELU) activation function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> SELU activation\n",
    "function often significantly outperforms other activation functions for\n",
    "such neural nets (will selfnormalize: the output of each layer will tend to preserve a mean of 0 and \n",
    "standard deviation of 1 during training, which solves the\n",
    "vanishing/exploding gradients problem) [The network’s architecture must be sequential. Unfortunately, if you\n",
    "try to use SELU in nonsequential architectures, such as recurrent networks will not necessarily outperform other\n",
    "activation functions]\n",
    "\n",
    "> - The input features must be standardized (mean 0 and standard deviation 1).\n",
    "> - Every hidden layer’s weights must be initialized with LeCun normal\n",
    "initialization. In Keras, this means setting\n",
    "kernel_initializer=\"lecun_normal\" .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in general SELU > ELU > leaky\n",
    "ReLU (and its variants) > ReLU > tanh > logistic\n",
    "- If the network’s architecture prevents it from self-normalizing, then ELU\n",
    "-  care a lot about runtime latency, then you may\n",
    "prefer leaky ReLU. (0.3 for leaky ReLU)\n",
    "- If you have spare time and computing power, you can use cross-validation to evaluate other\n",
    "activation functions, such as RReLU if your network is overfitting or PReLU if you\n",
    "have a huge training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> if speed is your priority, ReLU might still be the best choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"leak.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['deserialize',\n",
       " 'elu',\n",
       " 'exponential',\n",
       " 'gelu',\n",
       " 'get',\n",
       " 'hard_sigmoid',\n",
       " 'linear',\n",
       " 'relu',\n",
       " 'selu',\n",
       " 'serialize',\n",
       " 'sigmoid',\n",
       " 'softmax',\n",
       " 'softplus',\n",
       " 'softsign',\n",
       " 'swish',\n",
       " 'tanh']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[activation for activation in dir(keras.activations) if not activation.startswith(\"_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LeakyReLU', 'PReLU', 'ReLU', 'ThresholdedReLU']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[layer for layer in dir(keras.layers) if \"relu\" in layer.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.dense.Dense at 0x273c0cdf460>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for use SELU\n",
    "layer = keras.layers.Dense(10, activation=\"selu\", kernel_initializer=\"lecun_normal\")\n",
    "layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization\n",
    "scale it to 0 mean and 1 variance and  scale it add plus Bias term(called offset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although using He initialization along with ELU (or any variant of ReLU) can significantly reduce the danger of the vanishing/exploding gradients problems ( doesn’t guarantee that they\n",
    "won’t come back during training.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adding an operation in the model just before or after the activation function of each hidden layer\n",
    "\n",
    "<img src=\"bn.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source : https://www.youtube.com/watch?v=yXOMHOpbon8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the operation lets the model learn the optimal\n",
    "scale and mean of each of the layer’s inputs.\n",
    "\n",
    "if you add a\n",
    "BN layer as the very first layer of your neural network, you do not need to\n",
    "standardize your training set (The vanishing gradients problem was\n",
    "strongly reduced, to the point that they could use saturating activation\n",
    "functions such as the tanh and even the logistic activation function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, there is a runtime penalty: the neural network makes slower predictions due to the extra computations required at\n",
    "each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.8351579  0.694469   0.09321904]\n",
      " [0.12849092 0.86160636 0.40815735]\n",
      " [0.7946682  0.8283142  0.7110379 ]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "image_tensor = tf.random.uniform(shape=(3,3), minval=0, maxval=1)\n",
    "print(image_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.83474064 0.694122   0.09317247]\n",
      " [0.12842673 0.8611759  0.40795344]\n",
      " [0.7942712  0.82790035 0.71068263]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "bn_layers = keras.layers.BatchNormalization()(image_tensor)\n",
    "print(bn_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(), # do not to do manually normalize\n",
    "    keras.layers.Dense(300, activation=\"relu\" , use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, use_bias=False),\n",
    "    keras.layers.BatchNormalization(), # add BN before activation\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> the layer before a BatchNormalization layer does not need to have bias terms, since the BatchNormalization layer has some as well, it would be a waste of parameters, so you can set use_bias=False when creating those layers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The authors of the BN paper argued in favor of adding the BN layers before the activation functions, rather than after\n",
    "\n",
    "> you can experiment with this too to see which option works best on\n",
    "your dataset. To add the BN layers before the activation functions, you\n",
    "must remove the activation function from the hidden layers and add\n",
    "them as separate layers after the BN layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 784)              3136      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 300)               235200    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 300)              1200      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 100)               30000     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 100)              400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation (Activation)     (None, 100)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 270,946\n",
      "Trainable params: 268,578\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each BN layer adds four parameters per input: γ, β, μ, and\n",
    "σ (for example, the first BN layer adds 3,136 parameters, which is 4 × 784)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The last two parameters, μ and σ, are the moving averages; they are not\n",
    "affected by backpropagation, so Keras calls them “non-trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batch_normalization_1/gamma:0', True),\n",
       " ('batch_normalization_1/beta:0', True),\n",
       " ('batch_normalization_1/moving_mean:0', False),\n",
       " ('batch_normalization_1/moving_variance:0', False)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn1 = model.layers[1]\n",
    "[(var.name, var.trainable) for var in bn1.variables]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another popular technique to mitigate the exploding gradients problem is\n",
    "to clip the gradients during backpropagation so that they never exceed\n",
    "some threshold\n",
    "\n",
    "This technique is most\n",
    "often used in recurrent neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.optimizer_v2.gradient_descent.SGD at 0x273c0db5f90>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just a matter of setting the clipvalue or clipnorm argument when creating an optimizer\n",
    "keras.optimizers.SGD(clipvalue=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> clipvalue (float) is set, the gradient of each weight is clipped to be no higher than this value (maximum value set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reusing Pretrained Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "not a good idea to train a very large DNN from scratch( find an existing neural network that accomplishes a similar task to the one you are trying to tackle ) then reuse the lower layers of this\n",
    "network. This technique is called transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  transfer learning will work best when the inputs have similar low-level features.\n",
    "\n",
    "> The output layer of the original model should usually be replaced because may not have the right number of outputs for the new task.\n",
    "\n",
    "> the upper hidden layers of the original model are less likely to be as useful as the lower layers\n",
    "\n",
    "> It is also useful to reduce the learning rate when you unfreeze reused layers: this will avoid wrecking their fine-tuned weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model A is pretrained model then use taht transfer to model B\n",
    "model_A = keras.models.load_model('model_A.h5') # multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avoid affect of training (we B train will affect to A)\n",
    "model_A_clone = keras.models.clone_model(model_A)\n",
    "model_A_clone.set_weights(model_A.get_weights())\n",
    "model_B_on_A = keras.models.Sequential(model_A_clone.layers[:-1]) # transfer trained layer\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation='sigmoid')) # just binary class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze layers\n",
    "for layer in model_B_on_A.layers[:-1] :\n",
    "    layer.trainable = False # freeze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You must always compile your model after you freeze or unfreeze layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can train the model for a few epochs, then unfreeze the reused layers (which requires compiling the model again) and continue training\n",
    "to fine-tune the reused layers for task B. After unfreezing the reused layers, it is usually a good idea to reduce the learning rate, once again to\n",
    "avoid damaging the reused weights:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Transfer learning works\n",
    "best with deep convolutional neural networks, which tend to learn feature detectors that are much more general (especially in the lower layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you don’t have\n",
    "much labeled training data, you may still be able to perform *unsupervised pretraining*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can try to use it to train\n",
    "an unsupervised model, such as an autoencoder or a generative adversarial network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can reuse the lower layers of the\n",
    "autoencoder or the lower layers of the GAN’s discriminator, add the output layer for your task on top, and fine-tune the final network using supervised learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faster Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum Optimization\n",
    "Imagine a bowling ball rolling down a gentle slope on a smooth surface: it\n",
    "will start out slowly, but it will quickly pick up momentum until it eventually reaches terminal velocity.\n",
    "\n",
    "Momentum optimization cares a great deal about what previous gradients were: at each iteration (the gradient is used for acceleration, not for speed)\n",
    "\n",
    "prevent the momentum from growing too large, the algorithm introduces a new hyperparameter β, called\n",
    "the momentum, which must be set between 0 (high friction) and 1 (no friction). A typical momentum value is 0.9.\n",
    "\n",
    "momentum optimization to escape from plateaus much faster than Gradient\n",
    "Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "momentum = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9) # prevent overshoot and oscillate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> the momentum value of 0.9 usually works well in practice and almost always goes faster than regular Gradient Descent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nesterov Accelerated Gradient\n",
    "also known\n",
    "as Nesterov momentum optimization \n",
    "\n",
    "This small tweak works because in general the momentum vector will be\n",
    "pointing in the right direction (i.e., toward the optimum)\n",
    "\n",
    "NAG is generally faster than regular momentum optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nesterov = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaGrad\n",
    "It would be nice if the algorithm could correct its direction earlier to point a bit more toward the global optimum\n",
    "\n",
    "this algorithm decays the learning rate, but it does so faster for steep dimensions than for dimensions with gentler slopes. \n",
    "\n",
    "This is called\n",
    "an *adaptive learning rate*. It helps point the resulting updates more directly toward the global optimum.\n",
    "\n",
    "AdaGrad frequently performs well for simple quadratic problems, but it\n",
    "often stops too early when training neural networks. The learning rate\n",
    "gets scaled down so much that the algorithm ends up stopping.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSProp\n",
    "Root Mean Squared Propagation\n",
    "\n",
    "AdaGrad runs the risk of slowing down a bit too fast and\n",
    "never converging to the global optimum\n",
    "\n",
    "The RMSProp algorithm fixes\n",
    "this by accumulating only the gradients from the most recent iterations , It does so\n",
    "by using exponential decay in the first step\n",
    "\n",
    "The decay rate β is typically set to 0.9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsp = keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that the rho argument corresponds to β\n",
    "\n",
    "> In fact, it was the preferred optimization algorithm of\n",
    "many researchers until Adam optimization came around."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam and Nadam Optimization\n",
    "which stands for* adaptive moment estimation*, combines the\n",
    "ideas of momentum optimization and RMSProp:\n",
    "\n",
    "just like momentum optimization, it keeps track of an exponentially decaying average of past gradients; and just like RMSProp\n",
    "\n",
    "The momentum decay hyperparameter $β_1$ is typically initialized to 0.9,\n",
    "while the scaling decay hyperparameter $β_2$ is often initialized to 0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> it requires less tuning of the learning rate hyperparameter η.\n",
    "You can often use the default value η = 0.001, making Adam even easier to\n",
    "use than Gradient Descent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compares all the optimizers we’ve discussed so far (* is bad, **\n",
    "is average, and *** is good)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"optimizers.png\" width=400px/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you start with a\n",
    "large learning rate and then reduce it once training stops making fast\n",
    "progress (faster than with the optimal constant learning rate)\n",
    "\n",
    " It can also be beneficial to start with a low\n",
    "learning rate, increase it, then drop it again. These strategies are called\n",
    "learning schedules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power Scheduling\n",
    "\n",
    "lr = lr0 / (1 + steps / s)**c\n",
    "\n",
    "Keras uses c=1 and s = 1 / decay\n",
    "\n",
    "The learning rate drops\n",
    "at each step. After s steps, it is down to η / 2. After s more steps, it is\n",
    "down to η / 3, then it goes down to η / 4, then η / 5, and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizes = keras.optimizers.SGD(learning_rate=0.01, decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "n_epochs = 25\n",
    "lr0 = 0.01\n",
    "decay = 1e-4\n",
    "batch_size = 32\n",
    "n_steps_per_epoch = math.ceil(50000 / batch_size) # ceiling up decimal point\n",
    "epochs = np.arange(n_epochs)\n",
    "lrs = lr0 / (1 + decay * epochs * n_steps_per_epoch)\n",
    "\n",
    "plt.plot(epochs, lrs,  \"o-\")\n",
    "plt.axis([0, n_epochs - 1, 0, 0.01])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title(\"Power Scheduling\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponential scheduling\n",
    "lr = lr0 * 0.1**(epoch / s)\n",
    "\n",
    "The learning rate will gradually drop by a factor of 10 every s steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay_fn(epoch) :\n",
    "    return 0.01 * 0.1**(epoch / 20)\n",
    "\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If you want to update the learning rate at each iteration(per batch) rather than at each epoch, you must write your own callback class:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = keras.backend\n",
    "\n",
    "class ExponentialDecay(keras.callbacks.Callback) :\n",
    "    def __init__(self, s=40000) :\n",
    "        super().__init__()\n",
    "        self.s = s \n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None) :\n",
    "        # batch argument is reset at each epoch\n",
    "        lr = K.get_value(self.model.optimizer.learning_rate) # get current learning rate\n",
    "        K.set_value(self.model.optimizer.learning_rate, lr*0.1**(1/self.s)) # set new learning rate value\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None) :\n",
    "        logs = logs or  {}\n",
    "        logs['lr'] = K.get_value(self.model.optimizer.learning_rate) \n",
    "\n",
    "exp_decay = ExponentialDecay(s = 20 * len(X_train) // 32) # number of steps in 20 epochs (batch size = 32)\n",
    "history = model.fit(callbacks=[exp_decay])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance scheduling\n",
    "Measure the validation error every N steps (just like for early stopping), and reduce the learning rate by a factor of λ when the error\n",
    "stops dropping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.2, patience=5) # multiply 0.2 to lr if not improve loss for 5 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1cycle scheduling\n",
    "increase and decreas lr cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneCycleScheduler(keras.callbacks.Callback):\n",
    "    def __init__(self, iterations, max_rate, start_rate=None,\n",
    "                 last_iterations=None, last_rate=None):\n",
    "        self.iterations = iterations\n",
    "        self.max_rate = max_rate\n",
    "        self.start_rate = start_rate or max_rate / 10\n",
    "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "        self.last_rate = last_rate or self.start_rate / 1000\n",
    "        self.iteration = 0\n",
    "    def _interpolate(self, iter1, iter2, rate1, rate2):\n",
    "        return ((rate2 - rate1) * (self.iteration - iter1) / (iter2 - iter1) + rate1)\n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self.iteration < self.half_iteration:\n",
    "            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration,  self.max_rate, self.start_rate)\n",
    "        else:\n",
    "            rate = self._interpolate(2 * self.half_iteration, self.iterations, self.start_rate, self.last_rate)\n",
    "        self.iteration += 1\n",
    "        K.set_value(self.model.optimizer.learning_rate, rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 25\n",
    "onecycle = OneCycleScheduler(math.ceil(len('X_train') / batch_size) * n_epochs, max_rate=0.05)\n",
    "history = model.fit(callbacks=[onecycle])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoiding Overfitting Through Regularization\n",
    "great flexibility\n",
    "also makes the network prone to overfitting the training set. We need\n",
    "regularization.\n",
    "\n",
    "we will examine other popular\n",
    "regularization techniques for neural networks: ℓ and ℓ regularization,\n",
    "dropout, and max-norm regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $ℓ_1$ and $ℓ_2$ Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use $ℓ_2$ regularization to constrain a neural network’s connection weights, and/or $ℓ_1$\n",
    "regularization if you want a sparse model (with many weights equal to 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.dense.Dense at 0x273c51ed7b0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(100, kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "# or l1(0.1) for ℓ1 regularization with a factor of 0.1\n",
    "# or l1_l2(0.1, 0.01) for both ℓ1 and ℓ2 regularization, with factors 0.1 and 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you will typically want to apply the same regularizer to all layers in\n",
    "your network\n",
    "\n",
    "you may find yourself repeating the same arguments. This makes the code ugly and error-prone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "regularize_dense = partial(\n",
    "    keras.layers.Dense, activation='relu',\n",
    "    kernel_initializer='he_normal',\n",
    "    kernel_regularizer=keras.regularizers.l2(0.01)\n",
    ")\n",
    "\n",
    "model = keras.models.Sequential([ \n",
    "    keras.layers.Flatten(input_shape=(28,28)),\n",
    "    regularize_dense(200),\n",
    "    regularize_dense(100),\n",
    "    regularize_dense(10, activation='softmax', kernel_initializer='glorot_uniform'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "randomly dropping out nodes during training.\n",
    "\n",
    "it will be entirely ignored during this training step, but it may be active during the\n",
    "next step \n",
    "\n",
    "The hyperparameter p is called the dropout\n",
    "rate, and it is typically set between 10% and 50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([ \n",
    "    keras.layers.Flatten(input_shape=(28,28)),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(200, activation='relu'),\n",
    "    # ...\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In practice, you can usually apply dropout only to the neurons in the top one to three layers (excluding the output layer).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to regularize a self-normalizing network based on the SELU activation\n",
    "function, you should use alpha dropout (instead of regular dropout because regular dropout would break self normalization):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Monte Carlo (MC) Dropout**\n",
    "\n",
    "boost the performance of any trained dropout\n",
    "model without having to retrain it or even modify it at all, provides a\n",
    "much better measure of the model’s uncertainty, and is also amazingly\n",
    "simple to implement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we get an array with 100 output predictions for every test data we give. So we can run simple mean to see what the model thinks the output class should be on an average. We can also easily calculate the standard deviation in this sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probas = np.stack([model(X_test, training=True) for sample in range(100)])\n",
    "y_proba  = y_probas.mean(axis=0)\n",
    "y_std = y_probas.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that model(X) is similar to model.predict(X) except it returns a\n",
    "tensor rather than a NumPy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of Monte Carlo samples you use (100 in this example) is a hyperparameter you can tweak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your model contains other layers that behave in a special way during\n",
    "training (such as BatchNormalization layers), then you should not force you should replace the Dropout\n",
    "layers with the following MCDropout class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCDropout(keras.layers.Dropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)\n",
    "\n",
    "class MCAlphaDropout(keras.layers.AlphaDropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max-Norm Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ": Constrains the weights incident to each hidden unit to have a norm less than or equal to a desired value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.dense.Dense at 0x273c52c1600>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                   kernel_constraint=keras.constraints.max_norm(max_value=1., axis=0)) #the maximum norm value for the incoming weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ts of shape [number of inputs, number of\n",
    "neurons], so using axis=0 means that the max-norm constraint will apply independently to each neuron’s weight vector. If you want to use\n",
    "max-norm with convolutional layers , make sure to set\n",
    "the max_norm() constraint’s axis argument appropriately (usually\n",
    "axis=[0, 1, 2] ).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63e79917a05e390872358bfb73c58bc903ada01d2d04077091749088207d82cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
