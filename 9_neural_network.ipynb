{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Artificial Neural Network\n",
    "\n",
    "Birds inspired us to fly, Brain’s architecture inspirad to build an intelligent machine.\n",
    "\n",
    "ANN : is a Machine Learning model inspired by the networks of biological\n",
    "neurons found in our brains\n",
    "\n",
    "ANN are versatile, powerful, and scalable, making them ideal to tackle large and highly complex\n",
    "Machine Learning tasks : \n",
    "- classifying billions of images (e.g., Google Images)\n",
    "- powering speech recognition services (e.g., Apple’s Siri)\n",
    "- recommending the best videos to watch to hundreds of millions of users every day (e.g., YouTube)\n",
    "- learning to beat the world champion at the game of Go (DeepMind’s AlphaGo).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Biological to Artificial Neurons\n",
    "\n",
    "ANNs have been around for quite a while: they were first introduced back in 1943, McCulloch and Pitts presented a simplified computational model of how biological neurons might\n",
    "work together in animal brains to perform complex computations using\n",
    "propositional logic.\n",
    "\n",
    "In the early 1980s, new architectures were invented and better training techniques were developed, sparking a revival of interest in *connectionism*. But progress was slow, and by the 1990s other powerful Machine Learning techniques were invented, such as *Support Vector Machines* These techniques seemed to offer better results and stronger theoretical foundations than ANNs, so once again the study of neural networks was put on hold.\n",
    "\n",
    "The tremendous increase in computing power since the 1990s now\n",
    "makes it possible to train large neural networks in a reasonable\n",
    "amount of time  and ANNs frequently outperform other ML techniques on very\n",
    "large and complex problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biological Neurons\n",
    "Biological neurons produce short electrical impulses called action potentials (APs, or just signals) which travel along the axons and make the\n",
    "synapses release chemical signals called neurotransmitters. When a neuron receives a sufficient amount of these neurotransmitters within a few milliseconds, it fires its own electrical impulses. (some are inhibit), each neuron typically connected to thousands of other neurons.\n",
    "\n",
    "<img src=\"neuron_cell.PNG\" width=500px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logical Computations with Neurons\n",
    "McCulloch and Pitts proposed a very simple model also known as an *artificial neuron* : it has one or more binary(on/off) inputs and one binary output. the neuron activate when output has value more than a certain numbers of inputs are active.\n",
    "\n",
    "just simple like : and, or, not logic\n",
    "- and : outputs activate when input1 and input2 activate\n",
    "- or : outputs activate when input1 or input2 activate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Perceptron\n",
    "\n",
    "is one of the simplest ANN architectures, invented in 1957 by Frank Rosenblatt, slightly different from artificial neuron called a *threshold logic unit (TLU)* \n",
    "\n",
    "instead of binary on/off values, each input connect with a **weight** then sum of each input (Z = w1x1 + w2x2 + ... = wx.T), then applied step function to sum and outputs \n",
    "\n",
    "the result : $$h_w(X) = step(z)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the most common step function is the *Heaviside step juction* \n",
    "\n",
    "- heaviside(z) = 0 if z<0 else 1 (use threshold = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "computing the ouputs of a fully connected layer\n",
    "\n",
    "$$h_w,_b(X) = Φ(XW + b)$$\n",
    "\n",
    "- X is matrix of inputs features. 1 row per instance and 1 column per feature. ex ;(0.2, 0.3, 0.5, 1.2) of 1 instance have 4 feature.\n",
    "- W is a Weight ; has 1 row per inputs neuron\n",
    "- b is bias connect with Weight \n",
    "- the function Φ is called the *activation function* (step function in TLU)\n",
    "\n",
    "Perceptron learning rule (weight upfdate)\n",
    "- $$W_i,_j(nextstep) = W_i,_j + a(y_i - y_p)X_i$$\n",
    "\n",
    "a is a learning rate\n",
    "\n",
    "the decision boundary of each output neuron is linear, so Perceptrons are incapable with complex pattern (just like Logistic Regression Classifier)\n",
    "\n",
    "However, if traing instaces ate linearly seperable this algorithm would converge to a solution. This is called the *Perceptron Convergence Theorem*.\n",
    "\n",
    "for example with iris datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 2),\n",
       " (150,),\n",
       " array([[1.4, 0.2],\n",
       "        [1.4, 0.2],\n",
       "        [1.3, 0.2],\n",
       "        [1.5, 0.2],\n",
       "        [1.4, 0.2]]),\n",
       " array([1, 1, 1, 1, 1]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "Iris = load_iris()\n",
    "X = Iris['data'][:, (2,3)] # petal length and petal width\n",
    "y = (Iris['target'] == 0)*1 # True or False to 1 or 0 (Iris setosa?)\n",
    "X.shape, y.shape, X[:5], y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Perceptron_clf = Perceptron(max_iter=1000, tol=1e-3, random_state=42)\n",
    "Perceptron_clf.fit(X, y)\n",
    "\n",
    "y_pred = Perceptron_clf.predict([[2, 0.5]])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Perceptrons do not\n",
    "output a class probability; rather, they make predictions based on a hard\n",
    "threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"percepton_sep.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Multilayer Perceptron and Backpropagation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63e79917a05e390872358bfb73c58bc903ada01d2d04077091749088207d82cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
