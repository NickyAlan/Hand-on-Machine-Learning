{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Deep Neural Net (exercise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> if all the weights have the same initial\n",
    "value, even if that value is not zero, then symmetry is not broken (i.e., all neurons in a given layer are equivalent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A few advantages of the SELU function over the ReLU function are:\n",
    "> - can take negative values (help alleviate the vanishing gradient problem) \n",
    "> - has non-zero derivative (avoid the dying units)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> MC Dropout is exactly like dropout during\n",
    "training, but it is still active during inference, so each inference is\n",
    "slowed down slightly. More importantly, when using MC Dropout you\n",
    "generally want to run inference 10 times or more to get better predictions. This means that making predictions is slowed down by a factor\n",
    "of 10 or more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Build a DNN with 20 hidden layers of 100 neurons each (that's too many, but it's the point of this exercise). Use He initialization and the ELU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=(32,32,3)))\n",
    "for hidden_layer in range(20) :\n",
    "    model.add(keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Using Nadam optimization and early stopping, train the network on the CIFAR10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Nadam(learning_rate=5e-5)\n",
    "model.compile(loss=keras.losses.SparseCategoricalCrossentropy(), \n",
    "              optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 32, 32, 3), (50000, 1))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "X_train_full.shape, y_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40000, 32, 32, 3), (10000, 32, 32, 3))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_train_full[10000:]\n",
    "X_valid = X_train_full[:10000]\n",
    "y_train = y_train_full[10000:]\n",
    "y_valid = y_train_full[:10000]\n",
    "\n",
    "X_train.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the callbacks we need and train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint = keras.callbacks.ModelCheckpoint('save_model/my_cifar10.h5', save_best_only=True)\n",
    "run_index = 1 # increment every train the model\n",
    "run_logdir = os.path.join('logdir', 'my_cifar_10_logs', f'run_{run_index}')\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping, model_checkpoint, tensorboard_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-6b31ae3e16d165be\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-6b31ae3e16d165be\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./logdir/my_cifar_10_logs --port=6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2741a0ed5d0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=callbacks, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 1.4953 - accuracy: 0.4708\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.495286464691162, 0.4708000123500824]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.load_model('save_model/my_cifar10.h5') # load the best weihts\n",
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> with only 47% accuracy on Valid data, Let's see if we can improve performance using Batch Normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try adding Batch Normalization and compare the learning\n",
    "curves: Is it converging faster than before? Does it produce a better\n",
    "model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=(32, 32, 3)))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "for hidden_layer in range(20) :\n",
    "    model.add(keras.layers.Dense(100, kernel_initializer='he_normal'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation('elu'))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Nadam(learning_rate=5e-4) # change learning rate from experiment(this is the best after trained 20 epochs)\n",
    "model.compile(loss=keras.losses.SparseCategoricalCrossentropy(), optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint = keras.callbacks.ModelCheckpoint('save_model/my_cifar10_bn.h5', save_best_only=True) # change name\n",
    "run_index = 1 # increment every train the model\n",
    "run_logdir = os.path.join('logdir', 'my_cifar_10_logs', f'run_bn_{run_index}') #change path\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping, model_checkpoint, tensorboard_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 16s 13ms/step - loss: 0.9715 - accuracy: 0.6578 - val_loss: 1.3816 - val_accuracy: 0.5394\n",
      "Epoch 40/100\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 0.9600 - accuracy: 0.6597 - val_loss: 1.3955 - val_accuracy: 0.5347\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2742db16830>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=callbacks, batch_size=32) # add batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 1.3264 - accuracy: 0.5301\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.326360821723938, 0.5300999879837036]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.load_model('save_model/my_cifar10_bn.h5') # load best weight\n",
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - model converging faster 10 epochs (50 to 40 epochs)\n",
    "> - more accuray (53% from 47%)\n",
    "> - but slower time to training 11 min from 9 min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "replacing Batch Normalization with SELU, and make the necessary adjustements to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 11s 9ms/step - loss: 1.0302 - accuracy: 0.6490 - val_loss: 1.5343 - val_accuracy: 0.5032\n",
      "Epoch 22/100\n",
      "1250/1250 [==============================] - 12s 10ms/step - loss: 1.0033 - accuracy: 0.6598 - val_loss: 1.6453 - val_accuracy: 0.4765\n",
      "Epoch 23/100\n",
      "1250/1250 [==============================] - 12s 10ms/step - loss: 1.8493 - accuracy: 0.5366 - val_loss: 1.5707 - val_accuracy: 0.4668\n",
      "Epoch 24/100\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 1.2114 - accuracy: 0.5794 - val_loss: 1.5133 - val_accuracy: 0.4877\n",
      "Epoch 25/100\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 1.1460 - accuracy: 0.6040 - val_loss: 1.5513 - val_accuracy: 0.4875\n",
      "Epoch 26/100\n",
      "1250/1250 [==============================] - 11s 8ms/step - loss: 1.1007 - accuracy: 0.6220 - val_loss: 1.5317 - val_accuracy: 0.4891\n",
      "Epoch 27/100\n",
      "1250/1250 [==============================] - 11s 8ms/step - loss: 1.0667 - accuracy: 0.6330 - val_loss: 1.5719 - val_accuracy: 0.4931\n",
      "Epoch 28/100\n",
      "1250/1250 [==============================] - 11s 8ms/step - loss: 1.1030 - accuracy: 0.6203 - val_loss: 1.5924 - val_accuracy: 0.4924\n",
      "Epoch 29/100\n",
      "1250/1250 [==============================] - 11s 8ms/step - loss: 1.0311 - accuracy: 0.6465 - val_loss: 1.6089 - val_accuracy: 0.4919\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27431cc7340>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for hidden_layer in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=7e-4) # change some learning rate\n",
    "model.compile(loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# callbacks\n",
    "early_stopping = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint = keras.callbacks.ModelCheckpoint('save_model/my_cifar10_selu.h5', save_best_only=True) # change name\n",
    "run_index = 1 # increment every train the model\n",
    "run_logdir = os.path.join('logdir', 'my_cifar_10_logs', f'run_selu_{run_index}') #change path\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping, model_checkpoint, tensorboard_cb]\n",
    "\n",
    "# standardize input\n",
    "X_means = X_train.mean(axis=0)\n",
    "X_stds = X_train.std(axis=0)\n",
    "\n",
    "X_train_scaled = (X_train - X_means) / X_stds # use scaling from X_train to prevent data leaked\n",
    "X_valid_scaled = (X_valid - X_means) / X_stds\n",
    "X_test_scaled = (X_test - X_means) / X_stds\n",
    "\n",
    "# fit model\n",
    "model.fit(X_train_scaled, y_train, epochs=100, validation_data=(X_valid_scaled, y_valid), callbacks=callbacks, batch_size=32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 1.4797 - accuracy: 0.4906\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4797067642211914, 0.49059998989105225]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.load_model(\"save_model/my_cifar10_selu.h5\")\n",
    "model.evaluate(X_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> not as good as the model using batch normalization (53%), but it's convergence was faster (only 29 epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC Dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 12s 10ms/step - loss: 0.8851 - accuracy: 0.6982 - val_loss: 1.9240 - val_accuracy: 0.4975\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27433da20b0>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for hidden_layer in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "\n",
    "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=5e-4) # change learning rate\n",
    "model.compile(loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# callbacks\n",
    "early_stopping = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint = keras.callbacks.ModelCheckpoint('save_model/my_cifar10_alpha.h5', save_best_only=True) # change name\n",
    "run_index = 1 # increment every train the model\n",
    "run_logdir = os.path.join('logdir', 'my_cifar_10_logs', f'run_alpha_{run_index}') #change path\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping, model_checkpoint, tensorboard_cb]\n",
    "    \n",
    "\n",
    "# fit model\n",
    "model.fit(X_train_scaled, y_train, epochs=100, validation_data=(X_valid_scaled, y_valid), callbacks=callbacks, batch_size=32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 1.5190 - accuracy: 0.4641\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.5190484523773193, 0.4641000032424927]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.load_model(\"save_model/my_cifar10_alpha.h5\")\n",
    "model.evaluate(X_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use MC Dropout now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCAlphaDropout(keras.layers.AlphaDropout) :\n",
    "    def call(self, inputs) :\n",
    "        return super().call(inputs, training=True) # can perform on prediction part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a new model, identical to the one we just trained (with the same weights), but with MCAlphaDropout dropout layers instead of AlphaDropout layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_model = keras.models.Sequential([ \n",
    "    MCAlphaDropout(layer.rate) if isinstance(layer, keras.layers.AlphaDropout) else layer for layer in model.layers  \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create function to run model 10 (by default) times and return the mean prediction class proba then predict the most likely class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_dropout_probas(mc_model, X, n_samples=10) :\n",
    "    y_probas = [mc_model.predict(X) for sample in range(n_samples)]\n",
    "    return np.mean(y_probas, axis=0)\n",
    "\n",
    "def mc_dropot_pred_classes(mc_model, X, n_samples=10) :\n",
    "    y_probas = mc_dropout_probas(mc_model, X, n_samples)\n",
    "    return np.argmax(y_probas, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4628"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "y_pred = mc_dropot_pred_classes(mc_model, X_valid_scaled, 10)\n",
    "accuracy = np.mean(y_pred == y_valid[:, 0])\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We get no accuracy improvement in this case.\n",
    "\n",
    "> So the best model we got in this exercise is the Batch Normalization model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrain your model using 1cycle scheduling and see if it improves training speed and model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = keras.backend\n",
    "\n",
    "class OneCycleScheduler(keras.callbacks.Callback):\n",
    "    def __init__(self, iterations, max_rate, start_rate=None,\n",
    "                 last_iterations=None, last_rate=None):\n",
    "        self.iterations = iterations\n",
    "        self.max_rate = max_rate\n",
    "        self.start_rate = start_rate or max_rate / 10\n",
    "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "        self.last_rate = last_rate or self.start_rate / 1000\n",
    "        self.iteration = 0\n",
    "    def _interpolate(self, iter1, iter2, rate1, rate2):\n",
    "        return ((rate2 - rate1) * (self.iteration - iter1) / (iter2 - iter1) + rate1)\n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self.iteration < self.half_iteration:\n",
    "            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration,  self.max_rate, self.start_rate)\n",
    "        else:\n",
    "            rate = self._interpolate(2 * self.half_iteration, self.iterations, self.start_rate, self.last_rate)\n",
    "        self.iteration += 1\n",
    "        K.set_value(self.model.optimizer.learning_rate, rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for hidden_layer in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "\n",
    "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-2) # change learning rate\n",
    "model.compile(loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 9ms/step - loss: 0.9348 - accuracy: 0.6684 - val_loss: 1.5943 - val_accuracy: 0.5171\n",
      "Epoch 15/15\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.8945 - accuracy: 0.6839 - val_loss: 1.6145 - val_accuracy: 0.5167\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "batch_size = 128\n",
    "n_epochs = 15\n",
    "onecycle = OneCycleScheduler(math.ceil(len(X_train_scaled) / batch_size) * n_epochs, max_rate=0.05)\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs, batch_size=batch_size,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[onecycle])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One cycle allowed us to train the model in just 15 epochs, each taking only 2 seconds (thanks to the larger batch size). This is several times faster than the fastest model we trained so far. \n",
    "\n",
    "Moreover, we improved the model's performance (from 47% to 52%). The batch normalized model reaches a slightly better performance (53%), but it's much slower to train."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63e79917a05e390872358bfb73c58bc903ada01d2d04077091749088207d82cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
