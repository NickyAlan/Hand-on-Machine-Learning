{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.range(5)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset :\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1], shape=(2,), dtype=int32)\n",
      "tf.Tensor([2 3], shape=(2,), dtype=int32)\n",
      "tf.Tensor([4], shape=(1,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.batch(2)\n",
    "for item in dataset :\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "map function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 2], shape=(2,), dtype=int32)\n",
      "tf.Tensor([4 6], shape=(2,), dtype=int32)\n",
      "tf.Tensor([8], shape=(1,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(lambda z : z*2)\n",
    "for item in dataset :\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.unbatch()\n",
    "for item in dataset :\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.filter(lambda x : x > 3) # keep only > 3\n",
    "for item in dataset.take(2) : # get 2 sample\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([3 0 1 6 2 5], shape=(6,), dtype=int64)\n",
      "tf.Tensor([ 7  8  4 11  9 14], shape=(6,), dtype=int64)\n",
      "tf.Tensor([12 13 17 15 10 18], shape=(6,), dtype=int64)\n",
      "tf.Tensor([19 16], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(20)\n",
    "dataset = dataset.shuffle(buffer_size=4, seed=42).batch(6)\n",
    "for item in dataset :\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spliting the Data to CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_mean = scaler.mean_\n",
    "X_std = scaler.scale_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For a very large dataset that does not fit in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(data, name_prefix, header=None, n_parts=5) :\n",
    "    data_dir = os.path.join('dataset', 'housing')\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    path_format = os.path.join(data_dir, '_{}_{:02d}.csv')\n",
    "\n",
    "    filepath = []\n",
    "    m = len(data)\n",
    "    for file_id, row_ in enumerate(np.array_split(np.arange(m), n_parts)) : # like batch\n",
    "        part_csv = path_format.format(name_prefix, file_id)\n",
    "        filepath.append(part_csv)\n",
    "\n",
    "        with open(part_csv, 'wt', encoding='utf-8') as file : # write text mode\n",
    "            if header : \n",
    "                file.write(header)\n",
    "                file.write('\\n')\n",
    "            for row_id in row_ :\n",
    "                file.write(','.join([ repr(column) for column in data[row_id] ]))\n",
    "                file.write('\\n')\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map x and y\n",
    "train_data = np.c_[X_train, y_train]\n",
    "valid_data = np.c_[X_valid, y_valid]\n",
    "test_data = np.c_[X_test, y_test]\n",
    "header_cols = housing.feature_names + ['MedianHouseValue'] # x and y\n",
    "header = ','.join(header_cols)\n",
    "\n",
    "train_filepaths = save_to_csv(train_data, \"train\", header, n_parts=5)\n",
    "valid_filepaths = save_to_csv(valid_data, \"valid\", header, n_parts=3)\n",
    "test_filepaths = save_to_csv(test_data, \"test\", header, n_parts=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now let's take a peek at the first few lines of one of these CSV files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>MedianHouseValue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.5214</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.049945</td>\n",
       "      <td>1.106548</td>\n",
       "      <td>1447.0</td>\n",
       "      <td>1.605993</td>\n",
       "      <td>37.63</td>\n",
       "      <td>-122.43</td>\n",
       "      <td>1.442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.3275</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.490060</td>\n",
       "      <td>0.991054</td>\n",
       "      <td>3464.0</td>\n",
       "      <td>3.443340</td>\n",
       "      <td>33.69</td>\n",
       "      <td>-117.39</td>\n",
       "      <td>1.687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.1000</td>\n",
       "      <td>29.0</td>\n",
       "      <td>7.542373</td>\n",
       "      <td>1.591525</td>\n",
       "      <td>1328.0</td>\n",
       "      <td>2.250847</td>\n",
       "      <td>38.44</td>\n",
       "      <td>-122.98</td>\n",
       "      <td>1.621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.1736</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.289003</td>\n",
       "      <td>0.997442</td>\n",
       "      <td>1054.0</td>\n",
       "      <td>2.695652</td>\n",
       "      <td>33.55</td>\n",
       "      <td>-117.70</td>\n",
       "      <td>2.621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0549</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.312457</td>\n",
       "      <td>1.085092</td>\n",
       "      <td>3297.0</td>\n",
       "      <td>2.244384</td>\n",
       "      <td>33.93</td>\n",
       "      <td>-116.93</td>\n",
       "      <td>0.956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  3.5214      15.0  3.049945   1.106548      1447.0  1.605993     37.63   \n",
       "1  5.3275       5.0  6.490060   0.991054      3464.0  3.443340     33.69   \n",
       "2  3.1000      29.0  7.542373   1.591525      1328.0  2.250847     38.44   \n",
       "3  7.1736      12.0  6.289003   0.997442      1054.0  2.695652     33.55   \n",
       "4  2.0549      13.0  5.312457   1.085092      3297.0  2.244384     33.93   \n",
       "\n",
       "   Longitude  MedianHouseValue  \n",
       "0    -122.43             1.442  \n",
       "1    -117.39             1.687  \n",
       "2    -122.98             1.621  \n",
       "3    -117.70             2.621  \n",
       "4    -116.93             0.956  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.read_csv(train_filepaths[0]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an Input Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'dataset\\\\housing\\\\_train_00.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'dataset\\\\housing\\\\_train_03.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'dataset\\\\housing\\\\_train_01.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'dataset\\\\housing\\\\_train_04.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'dataset\\\\housing\\\\_train_02.csv', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42) # shuffle  \n",
    "for filepath in filepath_dataset :\n",
    "    print(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_reader = 4\n",
    "dataset = filepath_dataset.interleave( # like pipeline\n",
    "    lambda filepath : tf.data.TextLineDataset(filepath).skip(1), # skip line 1\n",
    "    cycle_length = n_reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.stack.imgur.com/iSldR.png\" width=600px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> cycle_length = numpy of reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'4.9803,29.0,5.33378196500673,1.0390309555854644,2104.0,2.831763122476447,33.83,-118.01,2.086'\n",
      "b'3.5081,23.0,6.169603524229075,1.1167400881057268,1318.0,2.9030837004405288,40.52,-122.33,1.167'\n",
      "b'3.5214,15.0,3.0499445061043287,1.106548279689234,1447.0,1.6059933407325193,37.63,-122.43,1.442'\n",
      "b'4.163,49.0,4.71830985915493,0.9894366197183099,660.0,2.323943661971831,37.54,-122.31,3.938'\n",
      "b'4.0729,4.0,6.163329161451815,1.1138923654568211,4546.0,2.8448060075093866,34.58,-118.03,1.543'\n",
      "b'4.4896,48.0,4.989304812834225,1.0623885918003566,1235.0,2.201426024955437,34.07,-118.38,5.00001'\n",
      "b'5.3275,5.0,6.490059642147117,0.9910536779324056,3464.0,3.4433399602385686,33.69,-117.39,1.687'\n",
      "b'4.5293,19.0,6.920661157024793,1.1305785123966943,1827.0,3.0198347107438015,38.34,-122.31,2.104'\n",
      "b'4.45,31.0,5.154377880184332,0.9930875576036866,1135.0,2.6152073732718892,33.94,-117.95,2.679'\n",
      "b'4.7411,35.0,4.759643916913946,0.9910979228486647,904.0,2.6824925816023737,34.36,-119.7,3.364'\n"
     ]
    }
   ],
   "source": [
    "for line in dataset.take(10) : # take 10 lines \n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=int32, numpy=1>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=nan>,\n",
       " <tf.Tensor: shape=(), dtype=float64, numpy=nan>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'hello'>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=5.0>]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record = [0, np.nan, tf.constant(np.nan, dtype=tf.float64), 'hello', tf.constant([])]\n",
    "parsed_fileds = tf.io.decode_csv('1,,,,5', record) # for missing value\n",
    "parsed_fileds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 8 # x\n",
    "\n",
    "@tf.function\n",
    "def preprocess(line) :\n",
    "    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)] # for missing value , and last one for y_missing\n",
    "    fileds = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    x = tf.stack(fileds[:-1]) # don't want y\n",
    "    y = tf.stack(fileds[-1])\n",
    "    return (x - X_mean) / X_std, y # return scaler and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
       " array([ 0.16579157,  1.216324  , -0.05204565, -0.39215982, -0.5277444 ,\n",
       "        -0.2633488 ,  0.8543046 , -1.3072058 ], dtype=float32)>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=2.782>)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> b' =  used to specify a bytes string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12  4  5]\n",
      " [ 1  4  1]]\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([12,4,5])\n",
    "b = tf.constant([1,4,1])\n",
    "print(tf.stack([a,b]).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_reader_dataset(filepaths, repeat=1, n_readers=5,\n",
    "                       n_read_threads=None, shuffle_buffer_size=10000,\n",
    "                       n_parse_threads=5, batch_size=32):\n",
    "    dataset = tf.data.Dataset.list_files(filepaths).repeat(repeat)\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "        cycle_length=n_readers, num_parallel_calls=n_read_threads)\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size) # how many rows to shuffle\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> dataset.prefetch(1) at the end of the pipeline (after batching). This will always prefetch one batch of data and make sure that there is always one ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x : [[ 2.2740278   0.6625668  -0.14016265 -0.32218233 -0.5277444  -0.4690744\n",
      "  -0.7197856   0.6268896 ]\n",
      " [-0.4166814   0.5834586  -0.27819967 -0.36343482 -0.23388447 -0.24151908\n",
      "   1.6413497  -1.0173439 ]\n",
      " [-0.63540334 -0.60316414 -0.1755314  -0.02318518 -0.04132406 -0.04645293\n",
      "  -0.6260894   0.24207076]\n",
      " [-0.29249796  0.02970133 -0.5297631  -0.32667488 -0.67558706 -0.08503588\n",
      "  -0.83690536  0.7518313 ]\n",
      " [-0.8063061   0.6625668  -0.64895284 -0.04169097 -0.30780578 -0.5935341\n",
      "  -0.87906855  0.70185536]]\n",
      "y : [4.302 0.74  1.84  1.988 3.25 ]\n",
      "-----------\n",
      "\n",
      "x : [[ 1.5568445e+00 -6.0316414e-01  7.2914946e-01 -3.1375077e-01\n",
      "  -9.6993186e-02  4.1983075e-02  1.0744886e+00 -1.1822641e+00]\n",
      " [-9.3845069e-01 -9.1959685e-01 -4.3922710e-01 -2.8386047e-01\n",
      "  -7.9331356e-01 -4.0453303e-01  1.8100007e+00 -1.3172033e+00]\n",
      " [-9.6353871e-01 -2.0762321e-01 -2.2058074e-01 -1.8000378e-01\n",
      "   9.3242931e-01  2.3953319e-01  9.0115184e-01 -6.6250986e-01]\n",
      " [ 6.6922909e-01 -2.1062195e+00  7.2275180e-01  4.4363844e-01\n",
      "   5.3667946e+00  2.9764706e-03 -8.2285154e-01  1.0317034e+00]\n",
      " [-9.0588719e-02  4.2524222e-01 -6.6905223e-02 -2.8314823e-02\n",
      "   2.3471152e-02  5.1656544e-01  8.0277163e-01 -1.1172957e+00]]\n",
      "y : [2.854 0.669 0.654 2.208 1.615]\n",
      "-----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "train_set = csv_reader_dataset(train_filepaths, batch_size=5)\n",
    "for x_batch, y_batch in train_set.take(2) :\n",
    "    print('x :', x_batch.numpy())\n",
    "    print('y :', y_batch.numpy())\n",
    "    print('-----------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = csv_reader_dataset(train_filepaths, repeat=None)\n",
    "valid_set = csv_reader_dataset(valid_filepaths)\n",
    "test_set = csv_reader_dataset(test_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "2322/2322 [==============================] - 6s 2ms/step - loss: 0.7898 - val_loss: 0.5738\n",
      "Epoch 2/4\n",
      "2322/2322 [==============================] - 4s 2ms/step - loss: 0.4794 - val_loss: 0.6693\n",
      "Epoch 3/4\n",
      "2322/2322 [==============================] - 6s 2ms/step - loss: 0.4350 - val_loss: 0.5052\n",
      "Epoch 4/4\n",
      "2322/2322 [==============================] - 6s 2ms/step - loss: 0.4184 - val_loss: 0.7185\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22652d84820>"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = tf.keras.models.Sequential([ \n",
    "    tf.keras.layers.Input(shape=(8,)),\n",
    "    tf.keras.layers.Dense(28, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(learning_rate=1e-3))\n",
    "\n",
    "model.fit(train_set, epochs=4,\n",
    "          validation_data=valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121/121 [==============================] - 0s 1ms/step - loss: 0.7185\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7185211777687073"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(valid_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63e79917a05e390872358bfb73c58bc903ada01d2d04077091749088207d82cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
